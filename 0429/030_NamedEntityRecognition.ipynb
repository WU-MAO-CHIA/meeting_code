{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"標頭\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 建立命名實體識別器\n",
    "### (NVIDIA NeMo v1.0)\n",
    "\n",
    "在此學習筆記中，您將建立可在醫療疾病摘要中尋找疾病名稱的 NER (命名實體識別) 應用程式。模型不會從清單中「搜尋」名稱，而是根據語言的背景資訊「識別」與疾病相關的特定詞彙。\n",
    "\n",
    "**[3.1 從命令列分類標記](#3.1-從命令列分類標記)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 資料輸入](#3.1.1-資料輸入)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1.1 IOB 標記](#3.1.1.1-IOB-標記)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 設定檔](#3.1.2-設定檔)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3 Hydra 支援的 Python 指令碼](#3.1.3-Hydra-支援的-Python-指令碼)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4 練習：訓練模型](#3.1.4-練習：訓練模型)<br>\n",
    "**[3.2 特定領域訓練](#3.2-特定領域訓練)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 使用 TensorBoard 將結果視覺化](#3.2.1-使用-TensorBoard-將結果視覺化)<br>\n",
    "**[3.3 評估](#3.3-評估)**<br>\n",
    "**[3.4 推論](#3.4-推論)**<br>\n",
    "\n",
    "在命名實體識別工作中，您將採用與文字分類工作相同的基本步驟來建立、訓練和測試專案。然而，這一次您將會在 *特定領域* 的 BioMegatron 語言模型上訓練分類器。BioMegatron 是類似 [BERT](https://arxiv.org/abs/1810.04805) 的 [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) 模型，已在大型生物醫學文字語料庫進行預先訓練 ([PubMed](https://pubmed.ncbi.nlm.nih.gov/) 摘要和商業用途文集的全文)。我們可以獲得比一般語言模型更好的效能，因為我們的疾病資料集來自相同的生物醫學領域。\n",
    "\n",
    "也有其他模型可以替代 BioMegatron，其中最著名的是 [BioBERT](https://arxiv.org/abs/1901.08746)。BioMegatron 與 BioBERT 相比，模型規模較大，而且已經在較大的文字語料庫進行預先訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為我們在「標記」層級分類，所以會使用 [標記分類](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html) 模型來進行命名實體識別，在此案例中是分類與疾病相關的詞彙。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "text_test.txt sample\n",
      "*****\n",
      "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor . \n",
      "The adenomatous polyposis coli ( APC ) tumour - suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK - 3beta ) , axin / conductin and betacatenin . \n",
      "Complex formation induces the rapid degradation of betacatenin . \n",
      "\n",
      "*****\n",
      "labels_test.txt sample\n",
      "*****\n",
      "O O O O O O O O B I I I O O \n",
      "O B I I I I I I O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      "O O O O O O O O O \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "NER3_DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "print(\"*****\\ntext_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/text_train.txt\n",
    "print(\"\\n*****\\nlabels_test.txt sample\\n*****\")\n",
    "!head -n 3 $NER3_DATA_DIR/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前不需要變更 `exp_manger` 預設設定值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 練習：訓練模型\n",
    "執行訓練指令碼 `token_classification_train.py`，方法與在文字分類學習筆記中進行的類似實驗相同。\n",
    "\n",
    "可供覆寫的新值已提供在下方的儲存格中。請使用適當的覆寫新增命令，並執行儲存格。如果遇到困難，請參閱 [解答](solutions/ex3.1.4.ipynb)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2022-04-29 11:26:09 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/token_classification_model/2022-04-29_11-26-09\n",
      "[NeMo I 2022-04-29 11:26:09 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2022-04-29 11:26:09 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 20\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: /dli/task/data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 64\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2022-04-29 11:26:09 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2022-04-29 11:26:09 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : /dli/task/data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2022-04-29 11:26:09 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2022-04-29 11:26:09 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2022-04-29 11:26:09 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2022-04-29 11:26:09 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2022-04-29 11:26:09 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2022-04-29 11:26:09 token_classification_utils:107] Class weights restored from /dli/task/data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2022-04-29 11:26:09 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:116] Setting Max Seq length to: 64\n",
      "[NeMo I 2022-04-29 11:26:24 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-04-29 11:26:24 data_preprocessing:301] Min: 4 |                  Max: 178 |                  Mean: 35.938237463126846 |                  Median: 34.0\n",
      "[NeMo I 2022-04-29 11:26:24 data_preprocessing:307] 75 percentile: 45.00\n",
      "[NeMo I 2022-04-29 11:26:24 data_preprocessing:308] 99 percentile: 88.77\n",
      "[NeMo W 2022-04-29 11:26:24 token_classification_dataset:145] 319 are longer than 64\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:150] subtokens: [CLS] I ##dent ##ification of AP ##C ##2 , a ho ##mo ##logue of the ad ##eno ##mat ##ous p ##oly ##po ##sis co ##li t ##umour suppress ##or . [SEP]\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_dataset:264] features saved to /dli/task/data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2022-04-29 11:26:24 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2022-04-29 11:26:25 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_dev.txt:\n",
      "[NeMo I 2022-04-29 11:26:25 data_preprocessing:135] label: 0, 22092 out of 23969 (92.17%).\n",
      "[NeMo I 2022-04-29 11:26:25 data_preprocessing:135] label: 2, 1090 out of 23969 (4.55%).\n",
      "[NeMo I 2022-04-29 11:26:25 data_preprocessing:135] label: 1, 787 out of 23969 (3.28%).\n",
      "[NeMo I 2022-04-29 11:26:25 token_classification_utils:101] Total labels: 23969. Label frequencies - {0: 22092, 2: 1090, 1: 787}\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:116] Setting Max Seq length to: 64\n",
      "[NeMo I 2022-04-29 11:26:28 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2022-04-29 11:26:28 data_preprocessing:301] Min: 4 |                  Max: 122 |                  Mean: 36.812567713976165 |                  Median: 34.0\n",
      "[NeMo I 2022-04-29 11:26:28 data_preprocessing:307] 75 percentile: 47.00\n",
      "[NeMo I 2022-04-29 11:26:28 data_preprocessing:308] 99 percentile: 83.56\n",
      "[NeMo W 2022-04-29 11:26:28 token_classification_dataset:145] 53 are longer than 64\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:148] *** Example ***\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:149] i: 0\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:150] subtokens: [CLS] BR ##CA ##1 is secret ##ed and exhibits properties of a g ##rani ##n . [SEP]\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:151] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:152] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:153] subtokens_mask: 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:155] labels: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:264] features saved to /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-04-29 11:26:28 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo W 2022-04-29 11:26:28 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [00:06<00:00, 62.3MB/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertEncoder: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-04-29 11:26:38 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 5e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-04-29 11:26:38 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f1fa156de20>\" \n",
      "    will be used during training (effective maximum steps = 3400) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 3400\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | bert_model            | BertEncoder          | 108 M \n",
      "1 | classifier            | TokenClassifier      | 592 K \n",
      "2 | loss                  | CrossEntropyLoss     | 0     \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.613   Total estimated model params size (MB)\n",
      "[NeMo W 2022-04-29 11:26:40 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.67it/s][NeMo I 2022-04-29 11:26:41 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         97.48       7.80      14.45       1487\n",
      "    B (label_id: 1)                                          3.03      14.29       5.00         42\n",
      "    I (label_id: 2)                                          5.89      96.20      11.09         79\n",
      "    -------------------\n",
      "    micro avg                                               12.31      12.31      12.31       1608\n",
      "    macro avg                                               35.47      39.43      10.18       1608\n",
      "    weighted avg                                            90.51      12.31      14.03       1608\n",
      "    \n",
      "[NeMo W 2022-04-29 11:26:41 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0:  85%|▊| 170/199 [00:14<00:02, 11.58it/s, loss=0.144, v_num=6-09, val_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|▊| 172/199 [00:14<00:02, 11.62it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 0:  88%|▉| 176/199 [00:14<00:01, 11.79it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 0:  90%|▉| 180/199 [00:15<00:01, 11.96it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 0:  92%|▉| 184/199 [00:15<00:01, 12.13it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 0:  94%|▉| 188/199 [00:15<00:00, 12.30it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 0:  96%|▉| 192/199 [00:15<00:00, 12.46it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 0:  98%|▉| 196/199 [00:15<00:00, 12.62it/s, loss=0.144, v_num=6-09, val_lo\u001b[A\n",
      "Validating:  90%|███████████████████████████▊   | 26/29 [00:00<00:00, 32.21it/s]\u001b[A[NeMo I 2022-04-29 11:26:57 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.13      98.95      99.04      21648\n",
      "    B (label_id: 1)                                         69.88      83.88      76.24        769\n",
      "    I (label_id: 2)                                         88.11      78.75      83.17       1073\n",
      "    -------------------\n",
      "    micro avg                                               97.54      97.54      97.54      23490\n",
      "    macro avg                                               85.71      87.19      86.15      23490\n",
      "    weighted avg                                            97.67      97.54      97.57      23490\n",
      "    \n",
      "Epoch 0: 100%|█| 199/199 [00:15<00:00, 12.69it/s, loss=0.144, v_num=6-09, val_lo\n",
      "                                                                                \u001b[AEpoch 0, global step 169: val_loss reached 0.09656 (best 0.09656), saving model to \"/dli/task/nemo_experiments/token_classification_model/2022-04-29_11-26-09/checkpoints/token_classification_model--val_loss=0.10-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  85%|▊| 170/199 [00:14<00:02, 11.51it/s, loss=0.0568, v_num=6-09, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|▊| 172/199 [00:14<00:02, 11.54it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Epoch 1:  88%|▉| 176/199 [00:15<00:01, 11.71it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Epoch 1:  90%|▉| 180/199 [00:15<00:01, 11.88it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Epoch 1:  92%|▉| 184/199 [00:15<00:01, 12.04it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Epoch 1:  94%|▉| 188/199 [00:15<00:00, 12.20it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Epoch 1:  96%|▉| 192/199 [00:15<00:00, 12.36it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Epoch 1:  98%|▉| 196/199 [00:15<00:00, 12.52it/s, loss=0.0568, v_num=6-09, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.94it/s]\u001b[A[NeMo I 2022-04-29 11:27:18 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.10      99.51      99.31      21648\n",
      "    B (label_id: 1)                                         88.14      87.91      88.02        769\n",
      "    I (label_id: 2)                                         94.21      86.39      90.13       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.54      98.54      98.54      23490\n",
      "    macro avg                                               93.81      91.27      92.49      23490\n",
      "    weighted avg                                            98.52      98.54      98.52      23490\n",
      "    \n",
      "Epoch 1: 100%|█| 199/199 [00:15<00:00, 12.59it/s, loss=0.0568, v_num=6-09, val_l\n",
      "                                                                                \u001b[AEpoch 1, global step 339: val_loss reached 0.06584 (best 0.06584), saving model to \"/dli/task/nemo_experiments/token_classification_model/2022-04-29_11-26-09/checkpoints/token_classification_model--val_loss=0.07-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  85%|▊| 170/199 [00:16<00:02, 10.27it/s, loss=0.0351, v_num=6-09, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|▊| 172/199 [00:16<00:02, 10.30it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Epoch 2:  88%|▉| 176/199 [00:16<00:02, 10.47it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Epoch 2:  90%|▉| 180/199 [00:16<00:01, 10.63it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Epoch 2:  92%|▉| 184/199 [00:17<00:01, 10.79it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Epoch 2:  94%|▉| 188/199 [00:17<00:01, 10.94it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Epoch 2:  96%|▉| 192/199 [00:17<00:00, 11.09it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Epoch 2:  98%|▉| 196/199 [00:17<00:00, 11.25it/s, loss=0.0351, v_num=6-09, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 32.09it/s]\u001b[A[NeMo I 2022-04-29 11:27:42 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.42      98.96      99.19      21648\n",
      "    B (label_id: 1)                                         81.97      91.03      86.26        769\n",
      "    I (label_id: 2)                                         88.33      89.56      88.94       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.27      98.27      98.27      23490\n",
      "    macro avg                                               89.90      93.18      91.46      23490\n",
      "    weighted avg                                            98.34      98.27      98.30      23490\n",
      "    \n",
      "Epoch 2: 100%|█| 199/199 [00:17<00:00, 11.32it/s, loss=0.0351, v_num=6-09, val_l\n",
      "                                                                                \u001b[AEpoch 2, global step 509: val_loss reached 0.07485 (best 0.06584), saving model to \"/dli/task/nemo_experiments/token_classification_model/2022-04-29_11-26-09/checkpoints/token_classification_model--val_loss=0.07-epoch=2.ckpt\" as top 3\n",
      "Epoch 3:  85%|▊| 170/199 [00:14<00:02, 11.40it/s, loss=0.0175, v_num=6-09, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  86%|▊| 172/199 [00:15<00:02, 11.44it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Epoch 3:  88%|▉| 176/199 [00:15<00:01, 11.61it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Epoch 3:  90%|▉| 180/199 [00:15<00:01, 11.78it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Epoch 3:  92%|▉| 184/199 [00:15<00:01, 11.96it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Epoch 3:  94%|▉| 188/199 [00:15<00:00, 12.12it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Epoch 3:  96%|▉| 192/199 [00:15<00:00, 12.29it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Epoch 3:  98%|▉| 196/199 [00:15<00:00, 12.46it/s, loss=0.0175, v_num=6-09, val_l\u001b[A\n",
      "Validating:  90%|███████████████████████████▊   | 26/29 [00:00<00:00, 33.81it/s]\u001b[A[NeMo I 2022-04-29 11:28:04 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.19      99.48      99.33      21648\n",
      "    B (label_id: 1)                                         86.83      88.30      87.56        769\n",
      "    I (label_id: 2)                                         94.18      87.51      90.72       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.57      98.57      98.57      23490\n",
      "    macro avg                                               93.40      91.76      92.54      23490\n",
      "    weighted avg                                            98.56      98.57      98.55      23490\n",
      "    \n",
      "Epoch 3: 100%|█| 199/199 [00:15<00:00, 12.53it/s, loss=0.0175, v_num=6-09, val_l\n",
      "                                                                                \u001b[AEpoch 3, global step 679: val_loss reached 0.08662 (best 0.06584), saving model to \"/dli/task/nemo_experiments/token_classification_model/2022-04-29_11-26-09/checkpoints/token_classification_model--val_loss=0.09-epoch=3.ckpt\" as top 3\n",
      "Epoch 4:  85%|▊| 170/199 [00:14<00:02, 11.56it/s, loss=0.0128, v_num=6-09, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  86%|▊| 172/199 [00:14<00:02, 11.59it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Epoch 4:  88%|▉| 176/199 [00:14<00:01, 11.76it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Epoch 4:  90%|▉| 180/199 [00:15<00:01, 11.94it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Epoch 4:  92%|▉| 184/199 [00:15<00:01, 12.11it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Epoch 4:  94%|▉| 188/199 [00:15<00:00, 12.27it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Epoch 4:  96%|▉| 192/199 [00:15<00:00, 12.43it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Epoch 4:  98%|▉| 196/199 [00:15<00:00, 12.59it/s, loss=0.0128, v_num=6-09, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.85it/s]\u001b[A[NeMo I 2022-04-29 11:28:26 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         98.98      99.73      99.35      21648\n",
      "    B (label_id: 1)                                         87.38      85.57      86.47        769\n",
      "    I (label_id: 2)                                         96.76      83.50      89.64       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.52      98.52      98.52      23490\n",
      "    macro avg                                               94.38      89.60      91.82      23490\n",
      "    weighted avg                                            98.50      98.52      98.49      23490\n",
      "    \n",
      "Epoch 4: 100%|█| 199/199 [00:15<00:00, 12.65it/s, loss=0.0128, v_num=6-09, val_l\n",
      "                                                                                \u001b[AEpoch 4, step 849: val_loss was not in top 3\n",
      "Epoch 5:  85%|▊| 170/199 [00:14<00:02, 11.59it/s, loss=0.0108, v_num=6-09, val_l\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  86%|▊| 172/199 [00:14<00:02, 11.62it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Epoch 5:  88%|▉| 176/199 [00:14<00:01, 11.80it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Epoch 5:  90%|▉| 180/199 [00:15<00:01, 11.97it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Epoch 5:  92%|▉| 184/199 [00:15<00:01, 12.14it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Epoch 5:  94%|▉| 188/199 [00:15<00:00, 12.31it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Epoch 5:  96%|▉| 192/199 [00:15<00:00, 12.47it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Epoch 5:  98%|▉| 196/199 [00:15<00:00, 12.64it/s, loss=0.0108, v_num=6-09, val_l\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.62it/s]\u001b[A[NeMo I 2022-04-29 11:28:45 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.23      99.57      99.40      21648\n",
      "    B (label_id: 1)                                         89.04      87.65      88.34        769\n",
      "    I (label_id: 2)                                         94.47      89.10      91.70       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.70      98.70      98.70      23490\n",
      "    macro avg                                               94.24      92.10      93.15      23490\n",
      "    weighted avg                                            98.68      98.70      98.68      23490\n",
      "    \n",
      "Epoch 5: 100%|█| 199/199 [00:15<00:00, 12.71it/s, loss=0.0108, v_num=6-09, val_l\n",
      "                                                                                \u001b[AEpoch 5, step 1019: val_loss was not in top 3\n",
      "Epoch 6:  85%|▊| 170/199 [00:14<00:02, 11.44it/s, loss=0.015, v_num=6-09, val_lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  86%|▊| 172/199 [00:14<00:02, 11.48it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 6:  88%|▉| 176/199 [00:15<00:01, 11.65it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 6:  90%|▉| 180/199 [00:15<00:01, 11.83it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 6:  92%|▉| 184/199 [00:15<00:01, 12.00it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 6:  94%|▉| 188/199 [00:15<00:00, 12.17it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 6:  96%|▉| 192/199 [00:15<00:00, 12.33it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Epoch 6:  98%|▉| 196/199 [00:15<00:00, 12.50it/s, loss=0.015, v_num=6-09, val_lo\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.94it/s]\u001b[A[NeMo I 2022-04-29 11:29:04 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.28      99.36      99.32      21648\n",
      "    B (label_id: 1)                                         86.20      88.56      87.36        769\n",
      "    I (label_id: 2)                                         91.59      88.26      89.89       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.50      98.50      98.50      23490\n",
      "    macro avg                                               92.35      92.06      92.19      23490\n",
      "    weighted avg                                            98.50      98.50      98.49      23490\n",
      "    \n",
      "Epoch 6: 100%|█| 199/199 [00:15<00:00, 12.57it/s, loss=0.015, v_num=6-09, val_lo\n",
      "                                                                                \u001b[AEpoch 6, step 1189: val_loss was not in top 3\n",
      "Epoch 7:  85%|▊| 170/199 [00:14<00:02, 11.54it/s, loss=0.00936, v_num=6-09, val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  86%|▊| 172/199 [00:14<00:02, 11.56it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Epoch 7:  88%|▉| 176/199 [00:15<00:01, 11.73it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Epoch 7:  90%|▉| 180/199 [00:15<00:01, 11.90it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Epoch 7:  92%|▉| 184/199 [00:15<00:01, 12.06it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Epoch 7:  94%|▉| 188/199 [00:15<00:00, 12.22it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Epoch 7:  96%|▉| 192/199 [00:15<00:00, 12.38it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Epoch 7:  98%|▉| 196/199 [00:15<00:00, 12.54it/s, loss=0.00936, v_num=6-09, val_\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.53it/s]\u001b[A[NeMo I 2022-04-29 11:29:23 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.14      99.44      99.29      21648\n",
      "    B (label_id: 1)                                         84.48      89.21      86.78        769\n",
      "    I (label_id: 2)                                         94.62      85.18      89.65       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.45      98.45      98.45      23490\n",
      "    macro avg                                               92.75      91.27      91.91      23490\n",
      "    weighted avg                                            98.46      98.45      98.44      23490\n",
      "    \n",
      "Epoch 7: 100%|█| 199/199 [00:15<00:00, 12.61it/s, loss=0.00936, v_num=6-09, val_\n",
      "                                                                                \u001b[AEpoch 7, step 1359: val_loss was not in top 3\n",
      "Epoch 8:  85%|▊| 170/199 [00:14<00:02, 11.52it/s, loss=0.00226, v_num=6-09, val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  86%|▊| 172/199 [00:14<00:02, 11.55it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Epoch 8:  88%|▉| 176/199 [00:15<00:01, 11.72it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Epoch 8:  90%|▉| 180/199 [00:15<00:01, 11.89it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Epoch 8:  92%|▉| 184/199 [00:15<00:01, 12.05it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Epoch 8:  94%|▉| 188/199 [00:15<00:00, 12.22it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Epoch 8:  96%|▉| 192/199 [00:15<00:00, 12.38it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Epoch 8:  98%|▉| 196/199 [00:15<00:00, 12.54it/s, loss=0.00226, v_num=6-09, val_\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.97it/s]\u001b[A[NeMo I 2022-04-29 11:29:42 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.22      99.39      99.31      21648\n",
      "    B (label_id: 1)                                         85.06      88.82      86.90        769\n",
      "    I (label_id: 2)                                         93.40      87.05      90.11       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.48      98.48      98.48      23490\n",
      "    macro avg                                               92.56      91.75      92.10      23490\n",
      "    weighted avg                                            98.49      98.48      98.48      23490\n",
      "    \n",
      "Epoch 8: 100%|█| 199/199 [00:15<00:00, 12.60it/s, loss=0.00226, v_num=6-09, val_\n",
      "                                                                                \u001b[AEpoch 8, step 1529: val_loss was not in top 3\n",
      "Epoch 9:  85%|▊| 170/199 [00:14<00:02, 11.78it/s, loss=0.00129, v_num=6-09, val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  86%|▊| 172/199 [00:14<00:02, 11.81it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Epoch 9:  88%|▉| 176/199 [00:14<00:01, 11.98it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Epoch 9:  90%|▉| 180/199 [00:14<00:01, 12.15it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Epoch 9:  92%|▉| 184/199 [00:14<00:01, 12.32it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Epoch 9:  94%|▉| 188/199 [00:15<00:00, 12.48it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Epoch 9:  96%|▉| 192/199 [00:15<00:00, 12.64it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Epoch 9:  98%|▉| 196/199 [00:15<00:00, 12.80it/s, loss=0.00129, v_num=6-09, val_\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.40it/s]\u001b[A[NeMo I 2022-04-29 11:30:00 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.24      99.44      99.34      21648\n",
      "    B (label_id: 1)                                         85.64      89.21      87.39        769\n",
      "    I (label_id: 2)                                         93.59      87.14      90.25       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.54      98.54      98.54      23490\n",
      "    macro avg                                               92.83      91.93      92.33      23490\n",
      "    weighted avg                                            98.54      98.54      98.53      23490\n",
      "    \n",
      "Epoch 9: 100%|█| 199/199 [00:15<00:00, 12.86it/s, loss=0.00129, v_num=6-09, val_\n",
      "                                                                                \u001b[AEpoch 9, step 1699: val_loss was not in top 3\n",
      "Epoch 10:  85%|▊| 170/199 [00:14<00:02, 11.35it/s, loss=0.00139, v_num=6-09, val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  86%|▊| 172/199 [00:15<00:02, 11.37it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Epoch 10:  88%|▉| 176/199 [00:15<00:01, 11.54it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Epoch 10:  90%|▉| 180/199 [00:15<00:01, 11.71it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Epoch 10:  92%|▉| 184/199 [00:15<00:01, 11.87it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Epoch 10:  94%|▉| 188/199 [00:15<00:00, 12.04it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Epoch 10:  96%|▉| 192/199 [00:15<00:00, 12.20it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Epoch 10:  98%|▉| 196/199 [00:15<00:00, 12.35it/s, loss=0.00139, v_num=6-09, val\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.97it/s]\u001b[A[NeMo I 2022-04-29 11:30:20 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.28      99.42      99.35      21648\n",
      "    B (label_id: 1)                                         86.57      88.82      87.68        769\n",
      "    I (label_id: 2)                                         93.25      88.82      90.98       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.59      98.59      98.59      23490\n",
      "    macro avg                                               93.03      92.35      92.67      23490\n",
      "    weighted avg                                            98.59      98.59      98.59      23490\n",
      "    \n",
      "Epoch 10: 100%|█| 199/199 [00:16<00:00, 12.42it/s, loss=0.00139, v_num=6-09, val\n",
      "                                                                                \u001b[AEpoch 10, step 1869: val_loss was not in top 3\n",
      "Epoch 11:  85%|▊| 170/199 [00:15<00:02, 11.17it/s, loss=0.000392, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  86%|▊| 172/199 [00:15<00:02, 11.20it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Epoch 11:  88%|▉| 176/199 [00:15<00:02, 11.37it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Epoch 11:  90%|▉| 180/199 [00:15<00:01, 11.54it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Epoch 11:  92%|▉| 184/199 [00:15<00:01, 11.70it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Epoch 11:  94%|▉| 188/199 [00:15<00:00, 11.86it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Epoch 11:  96%|▉| 192/199 [00:15<00:00, 12.02it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Epoch 11:  98%|▉| 196/199 [00:16<00:00, 12.17it/s, loss=0.000392, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 31.63it/s]\u001b[A[NeMo I 2022-04-29 11:30:39 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.34      99.46      99.40      21648\n",
      "    B (label_id: 1)                                         86.28      89.99      88.10        769\n",
      "    I (label_id: 2)                                         94.08      88.82      91.37       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.66      98.66      98.66      23490\n",
      "    macro avg                                               93.23      92.75      92.96      23490\n",
      "    weighted avg                                            98.67      98.66      98.66      23490\n",
      "    \n",
      "Epoch 11: 100%|█| 199/199 [00:16<00:00, 12.24it/s, loss=0.000392, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 11, step 2039: val_loss was not in top 3\n",
      "Epoch 12:  85%|▊| 170/199 [00:15<00:02, 11.33it/s, loss=0.00104, v_num=6-09, val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  86%|▊| 172/199 [00:15<00:02, 11.36it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Epoch 12:  88%|▉| 176/199 [00:15<00:01, 11.53it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Epoch 12:  90%|▉| 180/199 [00:15<00:01, 11.69it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Epoch 12:  92%|▉| 184/199 [00:15<00:01, 11.85it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Epoch 12:  94%|▉| 188/199 [00:15<00:00, 12.02it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Epoch 12:  96%|▉| 192/199 [00:15<00:00, 12.19it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Epoch 12:  98%|▉| 196/199 [00:15<00:00, 12.35it/s, loss=0.00104, v_num=6-09, val\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.45it/s]\u001b[A[NeMo I 2022-04-29 11:30:58 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.34      99.39      99.37      21648\n",
      "    B (label_id: 1)                                         86.33      88.69      87.49        769\n",
      "    I (label_id: 2)                                         92.40      89.47      90.91       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.59      98.59      98.59      23490\n",
      "    macro avg                                               92.69      92.52      92.59      23490\n",
      "    weighted avg                                            98.59      98.59      98.59      23490\n",
      "    \n",
      "Epoch 12: 100%|█| 199/199 [00:16<00:00, 12.42it/s, loss=0.00104, v_num=6-09, val\n",
      "                                                                                \u001b[AEpoch 12, step 2209: val_loss was not in top 3\n",
      "Epoch 13:  85%|▊| 170/199 [00:14<00:02, 11.56it/s, loss=0.000757, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  86%|▊| 172/199 [00:14<00:02, 11.60it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Epoch 13:  88%|▉| 176/199 [00:14<00:01, 11.77it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Epoch 13:  90%|▉| 180/199 [00:15<00:01, 11.94it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Epoch 13:  92%|▉| 184/199 [00:15<00:01, 12.11it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Epoch 13:  94%|▉| 188/199 [00:15<00:00, 12.28it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Epoch 13:  96%|▉| 192/199 [00:15<00:00, 12.45it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Epoch 13:  98%|▉| 196/199 [00:15<00:00, 12.61it/s, loss=0.000757, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.55it/s]\u001b[A[NeMo I 2022-04-29 11:31:17 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.14      99.57      99.35      21648\n",
      "    B (label_id: 1)                                         88.33      86.61      87.46        769\n",
      "    I (label_id: 2)                                         94.16      87.23      90.57       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.58      98.58      98.58      23490\n",
      "    macro avg                                               93.88      91.14      92.46      23490\n",
      "    weighted avg                                            98.56      98.58      98.56      23490\n",
      "    \n",
      "Epoch 13: 100%|█| 199/199 [00:15<00:00, 12.68it/s, loss=0.000757, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 13, step 2379: val_loss was not in top 3\n",
      "Epoch 14:  85%|▊| 170/199 [00:15<00:02, 10.88it/s, loss=0.000577, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  86%|▊| 172/199 [00:15<00:02, 10.91it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Epoch 14:  88%|▉| 176/199 [00:15<00:02, 11.08it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Epoch 14:  90%|▉| 180/199 [00:15<00:01, 11.25it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Epoch 14:  92%|▉| 184/199 [00:16<00:01, 11.42it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Epoch 14:  94%|▉| 188/199 [00:16<00:00, 11.59it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Epoch 14:  96%|▉| 192/199 [00:16<00:00, 11.75it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Epoch 14:  98%|▉| 196/199 [00:16<00:00, 11.91it/s, loss=0.000577, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 34.14it/s]\u001b[A[NeMo I 2022-04-29 11:31:37 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.41      99.39      99.40      21648\n",
      "    B (label_id: 1)                                         85.80      90.38      88.03        769\n",
      "    I (label_id: 2)                                         92.76      89.56      91.13       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.65      98.65      98.65      23490\n",
      "    macro avg                                               92.66      93.11      92.85      23490\n",
      "    weighted avg                                            98.66      98.65      98.65      23490\n",
      "    \n",
      "Epoch 14: 100%|█| 199/199 [00:16<00:00, 11.99it/s, loss=0.000577, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 14, step 2549: val_loss was not in top 3\n",
      "Epoch 15:  85%|▊| 170/199 [00:16<00:02, 10.41it/s, loss=0.000433, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  86%|▊| 172/199 [00:16<00:02, 10.45it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Epoch 15:  88%|▉| 176/199 [00:16<00:02, 10.61it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Epoch 15:  90%|▉| 180/199 [00:16<00:01, 10.78it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Epoch 15:  92%|▉| 184/199 [00:16<00:01, 10.94it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Epoch 15:  94%|▉| 188/199 [00:16<00:00, 11.10it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Epoch 15:  96%|▉| 192/199 [00:17<00:00, 11.26it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Epoch 15:  98%|▉| 196/199 [00:17<00:00, 11.41it/s, loss=0.000433, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.23it/s]\u001b[A[NeMo I 2022-04-29 11:31:57 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.36      99.41      99.39      21648\n",
      "    B (label_id: 1)                                         85.54      89.99      87.71        769\n",
      "    I (label_id: 2)                                         93.44      88.91      91.12       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.62      98.62      98.62      23490\n",
      "    macro avg                                               92.78      92.77      92.74      23490\n",
      "    weighted avg                                            98.64      98.62      98.63      23490\n",
      "    \n",
      "Epoch 15: 100%|█| 199/199 [00:17<00:00, 11.49it/s, loss=0.000433, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 15, step 2719: val_loss was not in top 3\n",
      "Epoch 16:  85%|▊| 170/199 [00:14<00:02, 11.86it/s, loss=0.00015, v_num=6-09, val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  86%|▊| 172/199 [00:14<00:02, 11.89it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Epoch 16:  88%|▉| 176/199 [00:14<00:01, 12.07it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Epoch 16:  90%|▉| 180/199 [00:14<00:01, 12.24it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Epoch 16:  92%|▉| 184/199 [00:14<00:01, 12.42it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Epoch 16:  94%|▉| 188/199 [00:14<00:00, 12.59it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Epoch 16:  96%|▉| 192/199 [00:15<00:00, 12.76it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Epoch 16:  98%|▉| 196/199 [00:15<00:00, 12.92it/s, loss=0.00015, v_num=6-09, val\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 34.10it/s]\u001b[A[NeMo I 2022-04-29 11:32:15 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.34      99.45      99.40      21648\n",
      "    B (label_id: 1)                                         85.77      90.12      87.89        769\n",
      "    I (label_id: 2)                                         94.05      88.44      91.16       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.65      98.65      98.65      23490\n",
      "    macro avg                                               93.05      92.67      92.82      23490\n",
      "    weighted avg                                            98.65      98.65      98.64      23490\n",
      "    \n",
      "Epoch 16: 100%|█| 199/199 [00:15<00:00, 12.99it/s, loss=0.00015, v_num=6-09, val\n",
      "                                                                                \u001b[AEpoch 16, step 2889: val_loss was not in top 3\n",
      "Epoch 17:  85%|▊| 170/199 [00:14<00:02, 11.94it/s, loss=0.000142, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  86%|▊| 172/199 [00:14<00:02, 11.97it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Epoch 17:  88%|▉| 176/199 [00:14<00:01, 12.15it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Epoch 17:  90%|▉| 180/199 [00:14<00:01, 12.32it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Epoch 17:  92%|▉| 184/199 [00:14<00:01, 12.49it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Epoch 17:  94%|▉| 188/199 [00:14<00:00, 12.66it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Epoch 17:  96%|▉| 192/199 [00:14<00:00, 12.83it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Epoch 17:  98%|▉| 196/199 [00:15<00:00, 13.00it/s, loss=0.000142, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.57it/s]\u001b[A[NeMo I 2022-04-29 11:32:34 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.35      99.45      99.40      21648\n",
      "    B (label_id: 1)                                         85.80      90.38      88.03        769\n",
      "    I (label_id: 2)                                         94.06      88.54      91.21       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.65      98.65      98.65      23490\n",
      "    macro avg                                               93.07      92.79      92.88      23490\n",
      "    weighted avg                                            98.66      98.65      98.65      23490\n",
      "    \n",
      "Epoch 17: 100%|█| 199/199 [00:15<00:00, 13.07it/s, loss=0.000142, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 17, step 3059: val_loss was not in top 3\n",
      "Epoch 18:  85%|▊| 170/199 [00:14<00:02, 11.89it/s, loss=0.000358, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  86%|▊| 172/199 [00:14<00:02, 11.92it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Epoch 18:  88%|▉| 176/199 [00:14<00:01, 12.10it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Epoch 18:  90%|▉| 180/199 [00:14<00:01, 12.27it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Epoch 18:  92%|▉| 184/199 [00:14<00:01, 12.44it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Epoch 18:  94%|▉| 188/199 [00:14<00:00, 12.61it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Epoch 18:  96%|▉| 192/199 [00:15<00:00, 12.78it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Epoch 18:  98%|▉| 196/199 [00:15<00:00, 12.94it/s, loss=0.000358, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.41it/s]\u001b[A[NeMo I 2022-04-29 11:32:52 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.35      99.43      99.39      21648\n",
      "    B (label_id: 1)                                         85.61      90.51      87.99        769\n",
      "    I (label_id: 2)                                         93.77      88.44      91.03       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.64      98.64      98.64      23490\n",
      "    macro avg                                               92.91      92.79      92.80      23490\n",
      "    weighted avg                                            98.65      98.64      98.64      23490\n",
      "    \n",
      "Epoch 18: 100%|█| 199/199 [00:15<00:00, 13.02it/s, loss=0.000358, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 18, step 3229: val_loss was not in top 3\n",
      "Epoch 19:  85%|▊| 170/199 [00:14<00:02, 11.72it/s, loss=0.000266, v_num=6-09, va\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  86%|▊| 172/199 [00:14<00:02, 11.75it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Epoch 19:  88%|▉| 176/199 [00:14<00:01, 11.93it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Epoch 19:  90%|▉| 180/199 [00:14<00:01, 12.10it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Epoch 19:  92%|▉| 184/199 [00:14<00:01, 12.28it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Epoch 19:  94%|▉| 188/199 [00:15<00:00, 12.44it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Epoch 19:  96%|▉| 192/199 [00:15<00:00, 12.61it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Epoch 19:  98%|▉| 196/199 [00:15<00:00, 12.77it/s, loss=0.000266, v_num=6-09, va\u001b[A\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.35it/s]\u001b[A[NeMo I 2022-04-29 11:33:11 token_classification_model:177] \n",
      "    label                                                precision    recall       f1           support   \n",
      "    O (label_id: 0)                                         99.35      99.45      99.40      21648\n",
      "    B (label_id: 1)                                         85.70      90.38      87.97        769\n",
      "    I (label_id: 2)                                         93.96      88.44      91.12       1073\n",
      "    -------------------\n",
      "    micro avg                                               98.65      98.65      98.65      23490\n",
      "    macro avg                                               93.00      92.76      92.83      23490\n",
      "    weighted avg                                            98.66      98.65      98.65      23490\n",
      "    \n",
      "Epoch 19: 100%|█| 199/199 [00:15<00:00, 12.84it/s, loss=0.000266, v_num=6-09, va\n",
      "                                                                                \u001b[AEpoch 19, step 3399: val_loss was not in top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 19: 100%|█| 199/199 [00:18<00:00, 10.70it/s, loss=0.000266, v_num=6-09, va\n",
      "[NeMo W 2022-04-29 11:33:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 9.31 s, sys: 2.66 s, total: 12 s\n",
      "Wall time: 7min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 2 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "# Override the config values in the command line\n",
    "# FIXME\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果如何？您的記錄應該包含以下內容：\n",
    "\n",
    "```\n",
    "    label                                                precision    recall       f1           support   \n",
    "    O (label_id: 0)                                         99.34      99.35      99.34      21648\n",
    "    B (label_id: 1)                                         85.86      89.21      87.50        769\n",
    "    I (label_id: 2)                                         91.74      89.00      90.35       1073\n",
    "    -------------------\n",
    "    micro avg                                               98.54      98.54      98.54      23490\n",
    "    macro avg                                               92.31      92.52      92.40      23490\n",
    "    weighted avg                                            98.55      98.54      98.55      23490\n",
    "    \n",
    "Epoch 2: 100%|█| 199/199 [00:15<00:00, 12.45it/s, loss=0.0251, v_num=4-43, val_l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.2 特定領域訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嘗試另一個實驗，這次將 `model.language_model.pretrained_model_name` 覆寫為 `biomegatron-bert-345m-cased`。這是包含 3.45 億個參數的大型模型，因此執行時間會更長。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2022-04-29 12:12:21 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/token_classification_model/2022-04-29_12-12-21\n",
      "[NeMo I 2022-04-29 12:12:21 exp_manager:563] TensorboardLogger has been set up\n",
      "[NeMo I 2022-04-29 12:12:21 token_classification_train:109] Config: pretrained_model: null\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 30\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: token_classification_model\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    model:\n",
      "      label_ids: null\n",
      "      class_labels:\n",
      "        class_labels_file: label_ids.csv\n",
      "      dataset:\n",
      "        data_dir: /dli/task/data/NCBI_ner-3\n",
      "        class_balancing: null\n",
      "        max_seq_length: 64\n",
      "        pad_label: O\n",
      "        ignore_extra_tokens: false\n",
      "        ignore_start_end: false\n",
      "        use_cache: true\n",
      "        num_workers: 2\n",
      "        pin_memory: false\n",
      "        drop_last: false\n",
      "      train_ds:\n",
      "        text_file: text_train.txt\n",
      "        labels_file: labels_train.txt\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      validation_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      test_ds:\n",
      "        text_file: text_dev.txt\n",
      "        labels_file: labels_dev.txt\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        batch_size: 32\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: biomegatron-bert-345m-cased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      head:\n",
      "        num_fc_layers: 2\n",
      "        fc_dropout: 0.5\n",
      "        activation: relu\n",
      "        use_transformer_init: true\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 5.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "[NeMo I 2022-04-29 12:12:21 megatron_utils:274] Downloading from https://api.ngc.nvidia.com/v2/models/nvidia/biomegatron345mcased/versions/0/files/vocab.txt\n",
      "Downloading: 100%|█████████████████████████████| 762/762 [00:00<00:00, 1.09MB/s]\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 44.1MB/s]\n",
      "Downloading: 100%|███████████████████████████| 29.0/29.0 [00:00<00:00, 42.8kB/s]\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 46.8MB/s]\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_train.txt\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:90] Labels mapping {'O': 0, 'B': 1, 'I': 2} saved to : /dli/task/data/NCBI_ner-3/label_ids.csv\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:99] Three most popular labels in /dli/task/data/NCBI_ner-3/labels_train.txt:\n",
      "[NeMo I 2022-04-29 12:12:24 data_preprocessing:135] label: 0, 124452 out of 135701 (91.71%).\n",
      "[NeMo I 2022-04-29 12:12:24 data_preprocessing:135] label: 2, 6115 out of 135701 (4.51%).\n",
      "[NeMo I 2022-04-29 12:12:24 data_preprocessing:135] label: 1, 5134 out of 135701 (3.78%).\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:101] Total labels: 135701. Label frequencies - {0: 124452, 2: 6115, 1: 5134}\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:107] Class weights restored from /dli/task/data/NCBI_ner-3/labels_train_weights.p\n",
      "[NeMo W 2022-04-29 12:12:24 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_train.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:54] Processing /dli/task/data/NCBI_ner-3/labels_dev.txt\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:74] Using provided labels mapping {'O': 0, 'B': 1, 'I': 2}\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_utils:96] /dli/task/data/NCBI_ner-3/labels_dev_label_stats.tsv found, skipping stats calculation.\n",
      "[NeMo I 2022-04-29 12:12:24 token_classification_dataset:272] features restored from /dli/task/data/NCBI_ner-3/cached_text_dev.txt_BertTokenizer_64_28996_-1\n",
      "[NeMo W 2022-04-29 12:12:24 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo I 2022-04-29 12:12:24 megatron_utils:274] Downloading from https://api.ngc.nvidia.com/v2/models/nvidia/biomegatron345mcased/versions/0/files/MegatronBERT.pt\n",
      "  4% [..                                                ]  29401088 / 665251537"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 5-6 minutes to run\n",
    "   \n",
    "TOKEN_DIR = \"/dli/task/nemo/examples/nlp/token_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "PRETRAINED_MODEL_NAME = 'biomegatron-bert-345m-cased'\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "MAX_SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 32\n",
    "AMP_LEVEL = 'O1'\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "# Override the config values in the command line\n",
    "!python $TOKEN_DIR/token_classification_train.py \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若要在測試集上評估模型，就必須指定 `.nemo` 訓練模型的位置。每個實驗都會在有 `nemo_experiments` 時間戳記的目錄下執行結果。繼續往下探究，可以找到 `checkpoints` 資料夾，最後的 `token_classification_model.nemo` 就在其中。下一個儲存格會用一些 Python 邏輯來擷取模型清單，並找出最新的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "nemo_model_paths = glob.glob('nemo_experiments/token_classification_model/*/checkpoints/*.nemo')\n",
    "\n",
    "# Sort newest first\n",
    "nemo_model_paths.sort(reverse=True)\n",
    "print(\"The latest model is \\n{}\".format(nemo_model_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用以下幾種方式，在測試集上執行評估：\n",
    "1. 使用相同的覆寫執行 `token_classification_evaluate.py`，再加上 `pretrained_model` 的覆寫，且必須為 `.nemo` 格式。\n",
    "\n",
    "```text\n",
    "   !python $TOKEN_DIR/token_classification_evaluate.py \\\n",
    "        model.dataset.data_dir=$DATA_DIR \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.test_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        pretrained_model=$LATEST_MODEL\n",
    "```\n",
    "        \n",
    "2. 儲存已訓練模型的檢查點並執行 NeMo 方法來評估測試集，以便具現化模型。<br>\n",
    "   我們將會逐步說明此方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model by restoring from the .nemo checkpoint\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "LATEST_MODEL = nemo_model_paths[0]\n",
    "model = nemo_nlp.models.TokenClassificationModel.restore_from(LATEST_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3'\n",
    "OUTPUT_DIR = '/dli/task/nemo_experiments/token_classification_model/logs'\n",
    "model.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'text_test.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'labels_test.txt'),\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.4 推論\n",
    "若要在查詢清單中執行推論，請使用已載入 `add_predictions` 方法的相同模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Clustering of missense mutations in the ataxia - telangiectasia gene in a sporadic T - cell leukaemia . \",\n",
    "    \"Ataxia - telangiectasia ( A - T ) is a recessive multi - system disorder caused by mutations in the ATM gene at 11q22 - q23 ( ref . 3 ) . \",\n",
    "    \"The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A - T patients and has long been associated with chromosomal instability . \",\n",
    "    \"By analysing tumour DNA from patients with sporadic T - cell prolymphocytic leukaemia ( T - PLL ) , a rare clonal malignancy with similarities to a mature T - cell leukaemia seen in A - T , we demonstrate a high frequency of ATM mutations in T - PLL . \",\n",
    "    \"In marked contrast to the ATM mutation pattern in A - T , the most frequent nucleotide changes in this leukaemia were missense mutations . \",\n",
    "    \"These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM - related proteins in mouse , yeast and Drosophila . \",\n",
    "    \"The resulting amino - acid substitutions are predicted to interfere with ATP binding or substrate recognition . \",\n",
    "    \"Two of seventeen mutated T - PLL samples had a previously reported A - T allele . \",\n",
    "    \"In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . \",\n",
    "    \"Occasional missense mutations in ATM were also found in tumour DNA from patients with B - cell non - Hodgkins lymphomas ( B - NHL ) and a B - NHL cell line . \"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.add_predictions(queries, output_file='predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat predictions.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
